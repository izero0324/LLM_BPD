{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from sentence_transformers) (4.66.4)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Downloading torch-2.3.1-cp311-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (from sentence_transformers) (1.3.1)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (from sentence_transformers) (1.11.3)\n",
      "Collecting huggingface-hub>=0.15.1 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/lib/python3.11/site-packages (from sentence_transformers) (10.0.0)\n",
      "Collecting filelock (from huggingface-hub>=0.15.1->sentence_transformers)\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.15.1->sentence_transformers)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.10.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Collecting networkx (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n",
      "  Downloading safetensors-0.4.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Using cached sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "Downloading huggingface_hub-0.24.0-py3-none-any.whl (419 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.0/419.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.3.1-cp311-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp311-cp311-macosx_11_0_arm64.whl (410 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.3/410.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: safetensors, networkx, fsspec, filelock, torch, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed filelock-3.15.4 fsspec-2024.6.1 huggingface-hub-0.24.0 networkx-3.3 safetensors-0.4.3 sentence_transformers-3.0.1 tokenizers-0.19.1 torch-2.3.1 transformers-4.42.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../secret.json') as f:\n",
    "    data = json.load(f)\n",
    "pinecone_key = data[\"pinecone_api\"]\n",
    "pc = Pinecone(api_key=pinecone_key)\n",
    "\n",
    "# Check if the index exists, and if not, create a new one\n",
    "index_name = 'codenet-python800'\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(index_name, dimension=384, metric='cosine', #dimension 384 for all-miniLM-L12-v2\n",
    "            spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        ) )\n",
    "\n",
    "# Connect to your Pinecone index\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_files_to_df():\n",
    "    base_dir = '/Users/andrewyang/Desktop/workspace/turintech/LLM_BPD/Project_CodeNet_Python800'\n",
    "\n",
    "    data = []\n",
    "    index_number = 1\n",
    "\n",
    "    # Iterate over each folder in the base directory\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                if file.endswith('.py'):\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file_content:\n",
    "                        try:\n",
    "                            code = file_content.read()\n",
    "                            # Append the content along with the index to the data list\n",
    "                            data.append({'index': index_number, 'codes': code})\n",
    "                            index_number += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    # Create a DataFrame from the data list\n",
    "    df = pd.DataFrame(data, columns=['index', 'codes']).set_index('index')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_prepared!\n",
      "                                                   codes\n",
      "index                                                   \n",
      "1      n=int(input())\\na=list(map(int,input().split()...\n",
      "2      s = int(input())\\ns = input().rstrip().split('...\n",
      "3      def lo2(n):\\n    count = 0\\n    while n%2==0:\\...\n",
      "4      a = int(input())\\nb = list(map(int, input().sp...\n",
      "5      N = int(input())\\nA = list(map(int,input().spl...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load codeNet_python800\n",
    "df = python_files_to_df()\n",
    "print('data_prepared!')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7500/7500 [28:56<00:00,  4.32it/s] \n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')  # Example model, replace with your choice\n",
    "\n",
    "code_texts = df['codes'].tolist()\n",
    "\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(code_texts, show_progress_bar=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings'] = embeddings\n",
    "# Prepare the data for upload\n",
    "data_to_upload = list(zip(df.index, df['embeddings'], df['codes']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_to_upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error,  (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 22 Jul 2024 11:10:50 GMT', 'Content-Type': 'application/json', 'Content-Length': '116', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '17', 'x-pinecone-request-id': '7747599557177307485', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Metadata size is 203645 bytes, which exceeds the limit of 40960 bytes per vector\",\"details\":[]}\n",
      "\n",
      "error,  (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 22 Jul 2024 11:10:51 GMT', 'Content-Type': 'application/json', 'Content-Length': '116', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '20', 'x-pinecone-request-id': '4306047114874772378', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Metadata size is 203703 bytes, which exceeds the limit of 40960 bytes per vector\",\"details\":[]}\n",
      "\n",
      "error,  (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 22 Jul 2024 11:11:34 GMT', 'Content-Type': 'application/json', 'Content-Length': '116', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '19', 'x-pinecone-request-id': '1803079457615244050', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Metadata size is 203673 bytes, which exceeds the limit of 40960 bytes per vector\",\"details\":[]}\n",
      "\n",
      "error,  (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 22 Jul 2024 11:11:39 GMT', 'Content-Type': 'application/json', 'Content-Length': '116', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '28', 'x-pinecone-request-id': '864024778883034738', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Metadata size is 225076 bytes, which exceeds the limit of 40960 bytes per vector\",\"details\":[]}\n",
      "\n",
      "error,  (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 22 Jul 2024 11:11:46 GMT', 'Content-Type': 'application/json', 'Content-Length': '115', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '20', 'x-pinecone-request-id': '2119268370128633474', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Metadata size is 60064 bytes, which exceeds the limit of 40960 bytes per vector\",\"details\":[]}\n",
      "\n",
      "error,  (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 22 Jul 2024 11:12:46 GMT', 'Content-Type': 'application/json', 'Content-Length': '115', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '18', 'x-pinecone-request-id': '1140026396551366135', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Metadata size is 50432 bytes, which exceeds the limit of 40960 bytes per vector\",\"details\":[]}\n",
      "\n",
      "error,  (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 22 Jul 2024 11:13:14 GMT', 'Content-Type': 'application/json', 'Content-Length': '116', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '25', 'x-pinecone-request-id': '7975931512307574585', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Metadata size is 203627 bytes, which exceeds the limit of 40960 bytes per vector\",\"details\":[]}\n",
      "\n",
      "error,  (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 22 Jul 2024 11:17:27 GMT', 'Content-Type': 'application/json', 'Content-Length': '116', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '20', 'x-pinecone-request-id': '1889061116399648080', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Metadata size is 204768 bytes, which exceeds the limit of 40960 bytes per vector\",\"details\":[]}\n",
      "\n",
      "error,  (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 22 Jul 2024 11:19:05 GMT', 'Content-Type': 'application/json', 'Content-Length': '116', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '18', 'x-pinecone-request-id': '7583019745634208742', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Metadata size is 426661 bytes, which exceeds the limit of 40960 bytes per vector\",\"details\":[]}\n",
      "\n",
      "error,  (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 22 Jul 2024 11:22:13 GMT', 'Content-Type': 'application/json', 'Content-Length': '116', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '13', 'x-pinecone-request-id': '1150346766657336313', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Metadata size is 204999 bytes, which exceeds the limit of 40960 bytes per vector\",\"details\":[]}\n",
      "\n",
      "error,  Failed to connect; did you specify the correct index name?\n"
     ]
    }
   ],
   "source": [
    "# Function to divide data into chunks for batch processing\n",
    "def chunked_data(data, chunk_size):\n",
    "    \"\"\"Yield successive chunk_size chunks from data.\"\"\"\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        yield data[i:i + chunk_size]\n",
    "\n",
    "# Upload data in batches\n",
    "batch_size = 100\n",
    "for batch in chunked_data(data_to_upload, batch_size):\n",
    "    batch_to_upsert = [(str(id), vec, {'code': codes}) for id, vec, codes in batch]\n",
    "    try:\n",
    "        index.upsert(vectors=batch_to_upsert)\n",
    "    except Exception as e:\n",
    "        print('error, ', e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
